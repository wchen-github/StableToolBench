{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62544ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # azureml-core of version 1.0.72 or higher is required\n",
    "# # azureml-core of version 1.0.72 or higher is required\n",
    "# from azureml.core import Workspace, Dataset\n",
    "\n",
    "# subscription_id = '3f2ab3f5-468d-4ba7-bc14-9d3a9da4bcc5'\n",
    "# resource_group = 'TownsendAML4'\n",
    "# workspace_name = 'townsendws4'\n",
    "\n",
    "# workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "# dataset = Dataset.get_by_name(workspace, name='Orca3_v2_epoch_3')\n",
    "# dataset.download(target_path='/data/arindam/Orca3_v2_epoch_3', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5457540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32003, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32003, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os \n",
    "\n",
    "\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, HfArgumentParser, TrainingArguments\n",
    "from transformers import StoppingCriteriaList, StoppingCriteria\n",
    "\n",
    "\n",
    "class EOSStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        last_token = input_ids[0][-1]\n",
    "        for stop in self.stops:\n",
    "            if stop == last_token:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def to_prompt(messages):\n",
    "    text = ''\n",
    "    for message in messages:\n",
    "        text += f\"<|im_start|>{message['role']}\\n{message['content']}\\n<|im_end|>\\n\"\n",
    "    text += f\"<|im_start|>assistant\\n\"\n",
    "    return text\n",
    "\n",
    "local_rank = 0\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    \"../../orca3/Orca3_v2_epoch_3/\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model = model.to(local_rank)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ba81f6b-1f88-48dc-8aba-903d84688baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = \"\"\"\n",
    "You are AutoGPT, you can use many tools(functions) to do the following task.\n",
    "First I will give you the task description, and your task start.\n",
    "At each step, you need to give your thought to analyze the status now and what to do next, with a function call to actually excute your step. Your output should follow this format:\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take (name of the api)\n",
    "Action Input: the input to the action\n",
    "\n",
    "After the call, you will get the call result, and you are now in a new state.\n",
    "Then you will analyze your status now, then decide what to do next...\n",
    "After many (Thought-call) pairs, you finally perform the task, then you can give your finial answer. If you believe that you have obtained enough information (which can be judge from the history observations) that can answer the task, please call:\n",
    "Action: Finish\n",
    "Action Input: {{\"return_type\": \"give_answer\", \"final_answer\": your answer string}}.\n",
    "Remember: \n",
    "1.the state change is irreversible, you can't go back to one of the former state, if you want more information, output your request for additional information.\n",
    "2.All the thought is short, at most in 3-5 sentences.\n",
    "3.You can do more then one trys, so if your plan is to continusly try some conditions, you can do one of the conditions per try.\n",
    "4.ALWAYS call \"Finish\" function at the end of the task. And the final answer should contain enough information to show to the user,If you can't handle the task, or you find that function calls always fail(the function is not valid now), use function Finish->give_up_and_restart.\n",
    "\n",
    "Let's Begin!\n",
    "Specifically, you have access to the following APIs: \n",
    "[{'name': 'love_quote_for_olato_quotes', 'description': 'This is the subfunction for tool \"olato_quotes\", you can use this tool.The description of this function is: \"It shows random quotes\"', 'parameters': {'type': 'object', 'properties': {}, 'required': [], 'optional': []}}, {'name': 'success_quote_for_olato_quotes', 'description': 'This is the subfunction for tool \"olato_quotes\", you can use this tool.The description of this function is: \"It shows random quotes\"', 'parameters': {'type': 'object', 'properties': {}, 'required': [], 'optional': []}}, {'name': 'motivation_quote_for_olato_quotes', 'description': 'This is the subfunction for tool \"olato_quotes\", you can use this tool.The description of this function is: \"It shows random quotes\"', 'parameters': {'type': 'object', 'properties': {}, 'required': [], 'optional': []}}, {'name': 'Finish', 'description': 'If you believe that you have obtained a result that can answer the task, please call this function to provide the final answer. Alternatively, if you recognize that you are unable to proceed with the task in the current state, call this function to restart. Remember: you must ALWAYS call this function at the end of your attempt, and the only part that will be shown to the user is the final answer, so it should contain sufficient information.', 'parameters': {'type': 'object', 'properties': {'return_type': {'type': 'string', 'enum': ['give_answer', 'give_up_and_restart']}, 'final_answer': {'type': 'string', 'description': 'The final answer you want to give the user. You should have this field if \"return_type\"==\"give_answer\"'}}, 'required': ['return_type']}}]\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "I'm planning a surprise party for my best friend, and I want to include meaningful quotes in the decorations. Can you provide me with random love, success, and motivation quotes? It would be great to have quotes that can celebrate love, success, and inspire everyone at the party. Thank you so much for your help!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2506b2-2aa0-4a97-8eb4-1407182e6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"../../orca3/Orca3_v2_epoch_3/\",\n",
    "    model_max_length=5000,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "res =  []\n",
    "gen_args = {\"num_return_sequences\":1}\n",
    "# if data_args.do_sample:\n",
    "#     gen_args[\"top_p\"] = 0.95\n",
    "#     gen_args[\"temperature\"] = 0.7\n",
    "\n",
    "\n",
    "stop_word_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids([\"<|im_end|>\"])[0]]\n",
    "stopping_criteria = StoppingCriteriaList([EOSStoppingCriteria(stops=stop_word_ids)])\n",
    "\n",
    "messages = [{\"role\":\"system\", \"content\":sm}, {\"role\":\"user\", \"content\":prompt}]\n",
    "text = to_prompt(messages)\n",
    "input_ids = tokenizer(tokenizer.bos_token + text, add_special_tokens=False, padding=False,\n",
    "                               return_tensors='pt').input_ids\n",
    "input_ids = input_ids.to(local_rank)\n",
    "output_ids = model.generate(input_ids, max_length=5000, do_sample=False, use_cache=True,\n",
    "                                        stopping_criteria=stopping_criteria,  **gen_args)\n",
    "output = [tokenizer.decode(oid[len(input_ids[0]):], skip_special_tokens=True).strip() for oid in output_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525c79d0-9b4a-4efa-83ff-05aef0275cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The user has requested random quotes for different themes: love, success, and motivation. I have access to three subfunctions of the \"olato_quotes\" tool that can provide these quotes. I will start by retrieving a random love quote.\n",
      "\n",
      "Action: love_quote_for_olato_quotes\n",
      "Action Input: {}\n"
     ]
    }
   ],
   "source": [
    "print(output[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_toolbench_orca3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
